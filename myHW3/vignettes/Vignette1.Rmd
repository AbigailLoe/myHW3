---
title: "Vignette1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignettetest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(myHW3)
library(bench)
library(ggplot2)
```

# The data to input:
The data should be in the form of a simple linear regression, with all covariates you wish to regress upon should be included in the data.

For example, say my data has an $X_1, X_2, X_3$ covariate.

One also needs to input the observed responses in a $Y$ parameter.

First, some generated covariate values.
```{r}
X = matrix(rnorm(666), ncol = 3)
Y = sample(1:nrow(X)) +rnorm(nrow(X), mean = 0, sd = .2)
```


If I want to get the linear regression $E[Y] = \beta_0+\beta_1X_1+ \beta_2X_2+ \beta_3X_3$, then I run:
```{r}
model1 = linearModel(X, Y)
```

To test this compared to the base R command, first inpute the data in the correct form.

```{r}
dat = cbind.data.frame(X, Y)
names(dat) = c("x1", "x2", "x3", "y")
test1 = lm(y~., data = dat)
```

Now compare the results:

```{r}
summary(test1)
model1
```
Just even looking at the results, they are very similar. Coefficient estimates are almost identical. There is some slight deviation in the t-values which results in minor deviations in the p-values, but I chalk that up to a rounding error/rounding differnece in code.


## Equivalence Testing of the simple model
To test the equivalence of the p-value estimates:
```{r}
lmPval = summary(test1)$coefficients[,4]
names(lmPval) = NULL
all.equal(unlist(round(model1["p-value"], 3), use.names = FALSE),
          round(lmPval, 3))
```

To test the equivalence of the coefficient estimates:
```{r}
lmEst = summary(test1)$coefficients[,1]
names(lmEst) = NULL
all.equal(unlist(round(model1["Estimate"], 5), use.names = FALSE),
          round(lmEst, 5))
```

To test the equivalence of the t-values:

```{r}
lmT = summary(test1)$coefficients[,3]
names(lmT) = NULL
all.equal(unlist(round(model1["t-value"], 3), use.names = FALSE),
          round(lmT, 3))
```


To test standard deviation of the coefficient estimates:
```{r}
lmSHat = summary(test1)$coefficients[,2]
names(lmSHat) = NULL
all.equal(unlist(round(model1["Std. Error"], 5), use.names = FALSE),
          round(lmSHat, 5))
```

# A more complicated model.
If one wishes to build a more complicated regression, you simply need to input the correct design matrix. If my model is $E[Y] = \beta_0 +\beta_1 X_1 + \beta_2 X_2 +\beta_3 X_1\times X_2$, then:

```{r}
X2 = cbind(X[,1:2], X[,1]*X[,2])
model2 = linearModel(X2, Y)
test2 = lm(Y~x1*x2, data = dat)
summary(test2)$coefficients
model2
```

## Equivalence Testing of the interaction model

To test the equivalence of the p-value estimates:
```{r}
lmPval = summary(test2)$coefficients[,4]
names(lmPval) = NULL
all.equal(unlist(round(model2["p-value"], 3), use.names = FALSE),
          round(lmPval, 3))
```

To test the equivalence of the coefficient estimates:
```{r}
lmEst = summary(test2)$coefficients[,1]
names(lmEst) = NULL
all.equal(unlist(round(model2["Estimate"], 5), use.names = FALSE),
          round(lmEst, 5))
```

To test the equivalence of the t-values:

```{r}
lmT = summary(test2)$coefficients[,3]
names(lmT) = NULL
all.equal(unlist(round(model2["t-value"], 3), use.names = FALSE),
          round(lmT, 3))
```


To test standard deviation of the coefficient estimates:
```{r}
lmSHat = summary(test2)$coefficients[,2]
names(lmSHat) = NULL
all.equal(unlist(round(model2["Std. Error"], 5), use.names = FALSE),
          round(lmSHat, 5))
```

# Efficiency comparison
We now turn to comparing the efficiency of this package against the `summary(lm)$coefficients` command in `R`. We will use the `bench::mark()` command and package in R.

```{r}
X = matrix(rgamma(100, shape = 2), nrow = 100)
Y = (rnorm(100, mean = 5, sd = 2))

dat = cbind.data.frame(X, Y)
bench::mark( linearModel(X, Y),
      summary(lm(Y~X, data = dat))$coefficients, check = FALSE)

```
We can see from the above tibble that our function is faster for small n. It also stores less information.

This trend continues as $n$ increases. Below are 2 plots comparing the memory allocation and run times. The lines in red correspond to the new package, while the line in blue corresponds to the base `R` command.

```{r}
res = NULL
for(i in seq(from = 1, to = 5, by = .5)){
    #generate appropriate data
  X = matrix(rgamma(10^i, shape = 2), nrow = 10^i)
  Y = (rnorm(10^i, mean = 5, sd = 2)) 

  
  dat = cbind.data.frame(X, Y)
  #compare models
  fooMark = mark( linearModel(X, Y),
      summary(lm(Y~X, data = dat))$coefficients, check = FALSE)
  res$size = c(res$size, 10^i)
  #store results in data frame
  res$runTimeMine = c(res$runTimeMine, fooMark$median[1])
  res$runTimeBase = c(res$runTimeBase, fooMark$median[2])
  res$runMemMine = c(res$runMemMine, fooMark$mem_alloc[1])
  res$runMemBase = c(res$runMemBase, fooMark$mem_alloc[2])
}
res = as.data.frame(res)
colMeans(res)
p = ggplot2::ggplot(data = res)+geom_line(aes(x = size, y = runTimeMine), color = "red")+
  geom_line(aes(x = size, y = runTimeBase), color = "blue")+
  labs(x = "Run Time", y = "Number of observations")+
  ggtitle("Comparing Run Times"); p
q = ggplot2::ggplot(data = res)+geom_line(aes(x = size, y = runMemMine), color = "red")+
  geom_line(aes(x = size, y = runMemBase), color = "blue")+
  labs(x = "Allocation", y = "Number of observations")+
  ggtitle("Comparing Memory Allocation"); q
```



# Possible Errors and differences
If an over fit regression is attempted, `linearModel` will throw an error.
```{r}
X = matrix(rnorm(42), ncol = 6, nrow = 7)
Y= rnorm(7)
linearModel(X,Y)
```

It is also important to note that my estimation of standard errors will not match the estimation provided in the base R function when the model fit is too perfect. For example, if $Y= \beta_0+\beta_1X_1$ with no error, then the estimation procedures for standard errors differ. However, this is simply because variance estimates are unstable at these points. Possible later versions of this package could implement variance stabilizing transformations, or in the case that the fit is almost too perfect, induce some random noise (although from a statistical perspective this is incredibly not ideal).

```{r}
X = as.matrix(rpois(500, lambda = 2), ncol = 1)
Y = X*5
linearModel(X= X, Y=Y)
dat = cbind.data.frame(X,Y)
summary(lm(Y~X, data = dat))$coefficients
```


# Concluding Thoughts

`linearModel` as a package has many useful features. It runs faster on average than the base equivalent, and it requires less memory storage. However, the base R command has increased functionality compared to our alternative. Our package has the advantage of requiring a design matrix rather than an `R` formula, which could prove to be a useful pedagogical tool for students learning the theoretical underpinnings of OLSR.

# Appendix: Extra Datasets

# Quadratic term
We start by generating data that fits the model $E[Y]= \beta_0 + \pi X_1^2$.

```{r}
x1= rgamma(20, 1, 1)
x2 = rbeta(20, 3, 5)

Y = pi *x1*x1 +rnorm(20, 0, 5)

X = matrix(c(x1, x2), nrow = 20, ncol = 2)

linearModel(X, Y)
dat = cbind.data.frame(X, Y); names(dat) = c("X1", "X2")
m1 = lm(Y ~ X1+X2, data = dat)
summary(m1)$coefficients

X = matrix(x1*x1, nrow = 20)
linearModel(X, Y)
m2 = lm(Y~I(X1^2), data = dat)
summary(m2)$coefficients

```

# Independent X and Y, Y non-normal
Now we will generate some $X,Y$ pairings that are independent of each other.
```{r}
Y = matrix(rpois(500, lambda = 3), nrow = 500)
X = matrix(rnorm(500, mean = 3, sd = 1), nrow = 500)
dat = cbind.data.frame(Y, X)

linearModel(X, Y)
summary(lm(Y~X, data = dat))$coefficients
```


# Half of the observations have a significant association
The model that we use is $E[Y] = \beta_0 - 2 X$ for even entries of $X$, but $Y \perp X$ for odd entries of $X$. We can think of this as some type of latent variable that confounds $X$.
```{r}
X = matrix(rexp(200, rate = 1/4), nrow = 200)
Y = 3* X + rnorm( 200, mean = 0, sd = .2)
X[seq(from = 1, to = 200, by = 2)] = runif(length(seq(from = 1, to = 200, by = 2)), min = -50, max = 50)

dat = cbind.data.frame(Y, X)

linearModel(X, Y)
summary(lm(Y~X, data = dat))$coefficients
```


# Half of the covariates are significantly associated

```{r}
X= matrix(rgamma(400, .4, 7), ncol = 8, nrow = 50) 
Y = 3*X[, 1]+ .2 *X[,3] + 5 + rnorm(50, 2, 1)
dat = cbind.data.frame(X, Y)
linearModel(X, Y)
summary(lm(Y~., data = dat))$coefficients
```

# Categorical Data

$E[Y] = \beta_0 + \beta_1 \times I(X=2) + \beta_2 I(X=3)$
```{r}
X = matrix(sample(3, size = 500, replace = TRUE))
X2 = ifelse(X == 2, 1, 0)
X3 = ifelse(X==3, 1, 0)
Y = rnorm(500, 0, sd = 1) + .25 *X2 -3*X3
dat = cbind.data.frame(X2, X3, Y)

linearModel(matrix(c(X2, X3), nrow = 500), Y)
summary(lm(Y~., data = dat))$coefficients
```

